{"metadata":{"kernelspec":{"display_name":"ML_VU","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"},{"sourceId":12456318,"sourceType":"datasetVersion","datasetId":7857523}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/felixfaruix/titanic-randomforest?scriptVersionId=250184056\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Titanic - Machine Learning from Disaster","metadata":{}},{"cell_type":"markdown","source":"This notebook is part of a _**Kaggle Competition**_ for the _Titanic_ dataset. The dataset is divided into train and test, where our goal is to predict what passengers survived the sank. We will do it using machine learning techniques and data science approach on the training data, building a model trying to predict the survivals on the test data. ","metadata":{}},{"cell_type":"code","source":"\"\"\"Importing necessary libraries for the Titanic dataset analysis.\nThis script is part of a Kaggle competition notebook for the Titanic dataset.\"\"\"\n\nimport pandas as pd \nimport sklearn as sk\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport sys","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the Titanic dataset\n# Ensure the correct path to the dataset files\n# Adjust the path as necessary based on your directory structure\ntrain_data = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_data = pd.read_csv('/kaggle/input/titanic/test.csv')","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"_Head()_ tells us the first 5 rows of the dataset, which is useful to understand the structure and content of the data.\nAs we can see, the dataset contains information about passengers such as their survival status, class, name and other features.     \nThis is useful to understand the structure and content of the data, and to identify any **potential issues or missing values** in the dataset.\nWe get a quick overview of the dataset, including the number of rows and columns, and the data types of each column.\n\nMore information can be obtained using the _info()_ and _describe()_ methods. Info method is particularly useful to understand the data types and missing values in the dataset, while describe() method gives us a statistical summary of the numerical columns.","metadata":{}},{"cell_type":"code","source":"# First 5 rows of the training data\ntrain_data.head()\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get a quick overview of the dataset\ntrain_data.info()\ntrain_data.describe()\n\n\n# Parch shows the number of parents/children aboard, this one shows that most of the passengers were alone. ","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The dataset is missing some values, especially in the '**Age**' and '**Embarked**' columns.\nWe can also see that the 'Survived' column is the target variable, which we will use to train our model.\n\n _Describe()_ shows that most of the people did not survive, which is expected as the Titanic sank.\nThe '**Pclass**' column indicates the class of the ticket, which is also an important feature to consider. This one shows that most of the passengers were in the 3rd class, which is expected as it was the cheapest ticket.\n'**Parch**' column shows the number of parents/children aboard, pointing out that most of the passengers were alone. \n\nAs our next step, we will now plot the correlation heatmap indicating the degree of linkage between each feature.\n","metadata":{}},{"cell_type":"code","source":"# Converting Ticket column into a str, for better usage \ntrain_data[\"Ticket\"] = train_data[\"Ticket\"].astype(str)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting only int or float columns correlations\nnumeric_df = train_data.select_dtypes(include=[\"int64\", \"float64\"])\nsns.heatmap(numeric_df.corr(), cmap='YlGnBu')\nplt.title('Correlation Heatmap of Titanic Dataset')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Splitting our train set via stratified shuffle split from _sklearn_","metadata":{}},{"cell_type":"markdown","source":"Now we want to split our _train_data_ into train and test sets, but with preserved class proportions. \n\n- We shuffle our dataset randomly via **StratifiedShuffleSplit** from _sklearn_\n- Then we split it into train and test sets (80%/20%)\n- WHile preserving the distribution of different target variables in both sets (Survived, Pclass, Sex)\n\nThis is important when the class is imbalanced like in this case, where the survival rate is low. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size=0.2, random_state=42)\nfor train_indeces, test_indeces in split.split(train_data, train_data[[\"Survived\", \"Pclass\", \"Sex\"]]):\n    strat_train_set = train_data.loc[train_indeces]\n    strat_test_set = train_data.loc[test_indeces]","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We plot each distirbution across the two sets, in order to verify that they are actually equal across each different variable. ","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 10))\nfig.suptitle(\"Stratified Train/Test Distributions\", fontsize=16)\n\n# Row 1: Survived\naxes[0, 0].hist(strat_train_set[\"Survived\"], bins=2, color=\"skyblue\")\naxes[0, 0].set_title(\"Train: Survived\")\naxes[0, 1].hist(strat_test_set[\"Survived\"], bins=2, color=\"brown\")\naxes[0, 1].set_title(\"Test: Survived\")\n\n# Row 2: Sex\naxes[1, 0].hist(strat_train_set[\"Sex\"], bins=2, color=\"skyblue\")\naxes[1, 0].set_title(\"Train: Sex\")\naxes[1, 1].hist(strat_test_set[\"Sex\"], bins=2, color=\"brown\")\naxes[1, 1].set_title(\"Test: Sex\")\n\n# Row 3: Pclass\naxes[2, 0].hist(strat_train_set[\"Pclass\"], bins=3, color=\"skyblue\")\naxes[2, 0].set_title(\"Train: Pclass\")\naxes[2, 1].hist(strat_test_set[\"Pclass\"], bins=3, color=\"brown\")\naxes[2, 1].set_title(\"Test: Pclass\")\n\n# Clean up\nfor ax in axes.flat:\n    ax.set_xticks([0, 1, 2, 3])\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"Count\")\n\nplt.tight_layout(pad = 0.8, h_pad= 0.2, w_pad= 0.2)\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strat_train_set.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature Engineering ","metadata":{}},{"cell_type":"markdown","source":"For first, we need to fill the gaps in the Age column. \nThen, we will have to drop the name column, which is a string, not brining any value to our feature space. Then, we will also transform our '**Sex**' column into a binary variable (0: Male - 1: Female). \n\nWe will also drop '**Cabin**' column and transform into integers the values from the '**Embarked**' column ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n\nclass FeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, cols=(\"Name\", \"Ticket\", \"Cabin\")):\n        self.cols = list(cols)\n    def fit(self, X, y=None): return self\n    def transform(self, X):\n        return X.drop(columns=self.cols, errors=\"ignore\").copy()\n\nclass AgeImputer(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.imp = SimpleImputer(strategy=\"mean\")\n    def fit(self, X, y=None):\n        self.imp.fit(X[[\"Age\"]]);  return self\n    def transform(self, X):\n        X = X.copy()\n        age_imputed = self.imp.transform(X[[\"Age\"]])\n        X[\"Age\"] = age_imputed.ravel() \n        return X\n\nclass CategoricalMapper(BaseEstimator, TransformerMixin):\n    sex_map = {\"male\": 0, \"female\": 1, \"0\": 0, \"1\": 1}\n    emb_map = {\"C\": 1, \"Q\": 2, \"S\": 3, \"N\": 0,\n               \"1\": 1, \"2\": 2, \"3\": 3, \"0\": 0}\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # --- Sex (works if already int or str)\n        X[\"Sex\"] = (\n            X[\"Sex\"]\n            .astype(str)\n            .str.lower()\n            .map(self.sex_map)\n            .astype(\"Int64\")\n        )\n\n        # --- Embarked (works if already int or str / NaN)\n        X[\"Embarked\"] = (\n            X[\"Embarked\"]\n              .fillna(\"N\")\n              .astype(str)\n              .str.upper()\n              .map(self.emb_map)\n              .astype(\"Int64\")\n        )\n        return X\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npreprocess_pipe = Pipeline([\n    (\"drop\", FeatureDropper()),\n    (\"age\",  AgeImputer()),\n    (\"cat\",  CategoricalMapper()),\n])\nprint(\"TRAIN cols :\", list(strat_train_set.columns))\nprint(\"VAL   cols :\", list(strat_test_set.columns))\n# fit + transform on training split\nstrat_train_set = preprocess_pipe.fit_transform(strat_train_set)\n\n# transform ONLY on validation / test split\nstrat_test_set  = preprocess_pipe.transform(strat_test_set)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strat_train_set.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"strat_test_set.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We notice that the dataset is clean now. All the features have the same number of values. ","metadata":{}},{"cell_type":"markdown","source":"#### Training and Evaluation of the Model (Random Forest Classifier)","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nX_train = strat_train_set.drop(\"Survived\", axis=1)\ny_train = strat_train_set[\"Survived\"]\nX_val = strat_test_set.drop(\"Survived\", axis=1)\ny_val = strat_test_set[\"Survived\"]\n\n# Define the model\nrf_model = RandomForestClassifier(random_state=42)\n# Define the parameter grid for GridSearchCV\nparam_grid = [\n    {'n_estimators': [10, 100, 200, 500],\n    'max_depth': [None, 5, 10],\n    'min_samples_split': [3, 5, 10],\n    }]\ngrid_search = GridSearchCV(rf_model, param_grid, cv=3, scoring='accuracy', return_train_score=True) \ngrid_search.fit(X_train, y_train)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_rf_model = grid_search.best_estimator_\nval_accuracy = best_rf_model.score(X_val, y_val)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We are now plotting the scores: ","metadata":{}},{"cell_type":"code","source":"print(\"Best cross-validated accuracy (train folds):\", grid_search.best_score_)\nprint(\"Final accuracy on hold-out validation set (strat_test_set):\", val_accuracy)\nprint(\"Best parameters:\", grid_search.best_params_)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fill forward missing Fare values in test_data\ntest_data[\"Fare\"] = test_data[\"Fare\"].ffill()\ntest_data[\"Fare\"].isna().sum()  # Should be 0 if all missing values are filled","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data = preprocess_pipe.transform(test_data)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_data.info()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"len(test_data)      :\", len(test_data))        # should be 418\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = best_rf_model.predict(test_data)\nprint(\"len(test_predictions):\", len(test_predictions))# must be 418","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_predictions = best_rf_model.predict(test_data)\n\nsubmission = pd.DataFrame({\n    \"PassengerId\": test_data[\"PassengerId\"],\n    \"Survived\": test_predictions\n})\nsubmission.to_csv(\"submission.csv\", index=False, mode=\"w\")","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"check = pd.read_csv(\"submission.csv\")\nprint(\"rows on disk      :\", len(check))   ","metadata":{},"outputs":[],"execution_count":null}]}